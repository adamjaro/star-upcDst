# star-upcDst

![Bilby Stampede](https://cds.cern.ch/record/2288105/files/fig1.png)

## Overview:

*star-upcDst*, is a new framework mainly developed by Jarda Adam, to simplify analysis that are related to forward and UPC physics. One can use this framework in 2 major steps, or only the 2nd step if the PicoDst files are generated by the 1st step already:

- Step 1: Use MuDst files of one's interest as input, run so-called picoMaker to produce a picoDst file with information that are needed for one's analysis only. 
- Step 2: Use the result of step 1, the picoDst, to build your personal analysis tree or histograms. 

- *Step 3* (optional): Setup condor job for running large samples

## Step 1:

- Make a clean area on RACF and checkout the repository:

<pre><code> git clone https://github.com/adamjaro/star-upcDst.git </pre></code>

- Go to the main directory star-upcDst:

<pre><code> cd star-upcDst </pre></code>

- Setup the StRoot and build ( already include compiling ) by doing:

<pre><code> ./build-maker.sh </pre></code>

- Next few steps are setting a directory for your input test filelist (below the example is called "test.list"), modify the run script for your need, and then run it:

<pre><code> mkdir txt </pre></code>
<pre><code> cp [your_filelist] ./txt/test.list </pre></code>

- Modify script "RunFilterMaker.C", there are a few major changes for your own interest. 1, Modify the input and output name at the beginning, here the example uses "./txt/test.list" and "test.root", respectively. 2, Add/modify trigger IDs around line 51, where the 2nd and 3rd arguments can be left blank for all run numbers. 3, Specify 0,1,2 for "data","starsim MC", or "embedding MC", respectively.

<pre><code> anaMaker->addTriggerId(trigger id, runnumber_start, runnumber_end); </pre></code> 

or

<pre><code> anaMaker->addTriggerId(trigger id); </pre></code>

and 

<pre><code> anaMaker->setIsMC(0); </pre></code>

- Run it to produce the picoDst file

<pre><code> root4star -l RunFilterMaker.C </pre></code>

- The output should be "test.root".  


## Step 2:

- With "test.root", the picoDst file, as the input, one can perform simple analysis or run over all events to save information into a tree or histograms. To do that, one needs to compile the package and setup the link to the library. This can be simply done in a few steps:

Under the main directory (under star-upcDst), create a new directory,

<pre><code> mkdir build </pre></code>
<pre><code> cd build </code></pre>
<pre><code> cmake ../ </code></pre>
<pre><code> make </code></pre>

Done. To try an example, go to folder "examples":

<pre><code> cd ../examples </code></pre>

and remember to change the input file name to what has been just created, "test.root", for macro "make_pT.C". Then do, 

<pre><code> root -l run_make_pT.C </code></pre>


- For another example, it uses a C++ code (with ROOT libraries) to read the picoDst file and save output into a small ROOT tree or anything one wants to save. Instead of running it as a .C ROOT macro, this example build an executable and can run as a standard C++ code without using ROOT.

Again, under the main directory, 

<pre><code> cd ./examples/dstreader </code></pre>

make another directory for build,

<pre><code> mkdir build </code></pre>

before one compiles the code, the input file needs to be modified accordingly in "./examples/dstreader/src/AnalysisTemplate.cxx". Again, change it to the "test.root" that has just been created, then go back to directory for build,


<pre><code> cd build </code></pre>
<pre><code> cmake ../ </code></pre>
<pre><code> make </code></pre>

The executable will show up as "AnalysisTemplate", now run it:

<pre><code> ./AnalysisTemplate </code></pre>

It produces an output, "output.root". 

Now it is time for analysis!

## Step 3 (optional): 

There are two different ways to setup your condor jobs. Let's see two examples of submitting jobs for running *Step 1*. Remember, one has to make sure the run script has been setup correctly in *Step 1* if one uses this to produce picoDst files. 

### Example 1:
- MuDst files are on distributed disks and STAR catalog can find your dataset, then one can use Jarda's example as a template. Jarda's example is based on AuAu Run14 UPC events:

Go to directory "scheduler" and only modify "SubmitQueryList.sh":
1. Modify the "top" directory to where output goes, for example, one can just do, 

<pre><code> mkdir output </code></pre>
and then set your "top" directory to "...../output";

2. Modify "qlist" to be the dataset of interest, change "qnames" accordingly for names one wants to give;

The do:

<pre><code> ./SubmitQueryList.sh </code></pre>

One can check the job status (you can just do condor_q or) after modifying the "basedir" in "PrintStat.py":

<pre><code> python PrintStat.py </pre></code>

Finally, to clean up after all outputs are produced correctly, type:

<pre><code> ./clean.sh </pre></code>


### Example 2:
- MuDst files are transfered by someone privately, so it can use a filelist as input to submit condor jobs, then one can use Kong's example as a template. Kong's example is based on dAu Run16 UPC events:

Go to directory "scheduler_filelist" and only modify "SubmitQueryList.sh" with similar steps as *Example 1*:
1. Modify "jobname" to whatever you want;
2. Modify the path to your filelist;

Then do exactly the same as above:

<pre><code> ./SubmitQueryList.sh </code></pre>

One can check the job status (you can just do condor_q or) after modifying the "basedir" in "PrintStat.py":

<pre><code> python PrintStat.py </pre></code>

Finally, to clean up after all outputs are produced correctly, type:

<pre><code> ./clean.sh </pre></code>


## Contact:
Adam, Jaroslav: <JaroslavAdam@creighton.edu>








